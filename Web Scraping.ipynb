{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing team attributes, not teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usable Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell gathers URL ends that will be inputted into the subsequent cell in order to gather relevant data for EACH game that will be inputted into a Pandas DF object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/boxscores/201909050chi.htm',\n",
       " '/boxscores/201909080cle.htm',\n",
       " '/boxscores/201909080car.htm',\n",
       " '/boxscores/201909080phi.htm',\n",
       " '/boxscores/201909080nyj.htm',\n",
       " '/boxscores/201909080min.htm',\n",
       " '/boxscores/201909080mia.htm',\n",
       " '/boxscores/201909080jax.htm',\n",
       " '/boxscores/201909080sea.htm',\n",
       " '/boxscores/201909080sdg.htm',\n",
       " '/boxscores/201909080tam.htm',\n",
       " '/boxscores/201909080dal.htm',\n",
       " '/boxscores/201909080crd.htm',\n",
       " '/boxscores/201909080nwe.htm',\n",
       " '/boxscores/201909090nor.htm',\n",
       " '/boxscores/201909090rai.htm',\n",
       " '/boxscores/201909120car.htm',\n",
       " '/boxscores/201909150cin.htm',\n",
       " '/boxscores/201909150was.htm',\n",
       " '/boxscores/201909150rav.htm',\n",
       " '/boxscores/201909150pit.htm',\n",
       " '/boxscores/201909150oti.htm',\n",
       " '/boxscores/201909150nyg.htm',\n",
       " '/boxscores/201909150mia.htm',\n",
       " '/boxscores/201909150htx.htm',\n",
       " '/boxscores/201909150gnb.htm',\n",
       " '/boxscores/201909150det.htm',\n",
       " '/boxscores/201909150rai.htm',\n",
       " '/boxscores/201909150ram.htm',\n",
       " '/boxscores/201909150den.htm',\n",
       " '/boxscores/201909150atl.htm',\n",
       " '/boxscores/201909160nyj.htm']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = [2019]\n",
    "#years = range(2011, 2021)\n",
    "#weeks = range(1,22)\n",
    "weeks = [1,2]\n",
    "game_link_ends = []\n",
    "for year in years:\n",
    "    for week in weeks:\n",
    "        url0 = 'https://www.pro-football-reference.com/years/{}/week_{}.htm'.format(year, week)\n",
    "        response0 = requests.get(url0)\n",
    "        page0 = response0.text\n",
    "        soup0 = BeautifulSoup(page0)\n",
    "        for link in soup0.find_all(class_=\"right gamelink\"):\n",
    "            url1 = link.findNext()\n",
    "            game_link_ends0 = url1.get('href')  #.get() method and \n",
    "                                                #calling index by attr (url['href']) act the same!\n",
    "            game_link_ends.append(game_link_ends0.strip())\n",
    "list(game_link_ends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get to the actual game pages where we can extract data into our Pandas DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://www.pro-football-reference.com'\n",
    "url_list = []\n",
    "for game_link in game_link_ends:\n",
    "    url = ''.join([url2, game_link])\n",
    "    url_list.append(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n"
     ]
    }
   ],
   "source": [
    "#Sanity checks to make sure we get all the boxscore links & subsequent pages\n",
    "#print(*url_list, sep='\\n')\n",
    "print(len(url_list), len(game_link_ends))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are actually on the pages where we can grab info for a model. Let's start with total points scored for the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USEABLE CODE BELOW, TEST SCRAPING ON ONE GAME FOR NOW!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 56,\n",
       " 57,\n",
       " 59,\n",
       " 33,\n",
       " 40,\n",
       " 69,\n",
       " 66,\n",
       " 41,\n",
       " 54,\n",
       " 48,\n",
       " 52,\n",
       " 54,\n",
       " 36,\n",
       " 58,\n",
       " 40,\n",
       " 34,\n",
       " 58,\n",
       " 52,\n",
       " 40,\n",
       " 54,\n",
       " 36,\n",
       " 42,\n",
       " 43,\n",
       " 25,\n",
       " 37,\n",
       " 23,\n",
       " 38,\n",
       " 36,\n",
       " 30,\n",
       " 44,\n",
       " 26]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start with first game to get a sense of basic structure\n",
    "totals = []\n",
    "for url in url_list:\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    page = page.replace(\"<!--\",\"\").replace(\"-->\",\"\")\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "#scrape totals from scorebox at top of page\n",
    "    data = []\n",
    "    table = soup.find('table', class_=\"linescore nohover stats_table no_freeze\")\n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        #cols = [ele.text.strip() for ele in cols]\n",
    "        data.append([ele for ele in cols[-1] if ele]) # Get rid of empty values\n",
    "        total = sum([int(item) for sublist in data for item in sublist]) #flatten list to be able to add\n",
    "    totals.append(total)\n",
    "totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workable Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.pro-football-reference.com/boxscores/201909050chi.htm'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with first game to get a sense of basic structure; REMOVE COMMENTS\n",
    "response = requests.get(url_list[0])\n",
    "page = response.text\n",
    "page = page.replace(\"<!--\",\"\").replace(\"-->\",\"\")\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape totals from scorebox at top of page\n",
    "data = []\n",
    "table = soup.find('table', class_=\"linescore nohover stats_table no_freeze\")\n",
    "table_body = table.find('tbody')\n",
    "\n",
    "rows = table_body.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data.append([ele for ele in cols[-1] if ele]) # Get rid of empty values\n",
    "    total = sum([int(item) for sublist in data for item in sublist])\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Traditional Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['13', '16'],\n",
       " ['22-47-0', '15-46-0'],\n",
       " ['18-30-203-1-0', '26-45-228-0-1'],\n",
       " ['5-37', '5-20'],\n",
       " ['166', '208'],\n",
       " ['213', '254'],\n",
       " ['1-0', '0-0'],\n",
       " ['0', '1'],\n",
       " ['10-71', '10-107'],\n",
       " ['2-12', '3-15'],\n",
       " ['0-0', '0-2'],\n",
       " ['31:03', '28:57']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape team stats box for traditional metrics\n",
    "data1 = []\n",
    "table1 = soup.find('table', class_=\"add_controls stats_table\")\n",
    "table_body1 = table1.find('tbody')\n",
    "\n",
    "rows1 = table_body1.find_all('tr')\n",
    "for row in rows1:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data1.append([ele.text for ele in cols if ele]) # Get rid of empty values\n",
    "\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Traditional Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "#total first downs\n",
    "total_first_downs = sum(int(item) for item in data1[0])\n",
    "print(total_first_downs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "93\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#separate rush stats and make appropriate transformations\n",
    "rush_stats = [item.split('-') for item in data1[1]]\n",
    "total_rush_att = int(rush_stats[0][0]) + int(rush_stats[1][0])\n",
    "print(total_rush_att)\n",
    "total_rush_yds = int(rush_stats[0][1]) + int(rush_stats[1][1])\n",
    "print(total_rush_yds)\n",
    "total_rush_tds = int(rush_stats[0][2]) + int(rush_stats[1][2])\n",
    "print(total_rush_tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "75\n",
      "431\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#separate pass stats and make appropriate transformations\n",
    "pass_stats = [item.split('-') for item in data1[2]]\n",
    "#print(pass_stats)\n",
    "total_comp = int(pass_stats[0][0]) + int(pass_stats[1][0])\n",
    "print(total_comp)\n",
    "total_att = int(pass_stats[0][1]) + int(pass_stats[1][1])\n",
    "print(total_att)\n",
    "total_pass_yds = int(pass_stats[0][2]) + int(pass_stats[1][2])\n",
    "print(total_pass_yds)\n",
    "total_pass_tds = int(pass_stats[0][3]) + int(pass_stats[1][3])\n",
    "print(total_pass_tds)\n",
    "total_pass_int = int(pass_stats[0][4]) + int(pass_stats[1][4])\n",
    "print(total_pass_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "#seperate sacks and make appropriate transformations\n",
    "sack_stats = [item.split('-') for item in data1[3]]\n",
    "#print(sack_stats)\n",
    "total_sacks = int(sack_stats[0][0]) + int(sack_stats[1][0])\n",
    "print(total_sacks)\n",
    "total_sack_yds = int(sack_stats[0][1]) + int(sack_stats[1][1])\n",
    "print(total_sack_yds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374\n",
      "467\n"
     ]
    }
   ],
   "source": [
    "#net pass yards & total yards (stats that don't require splits)\n",
    "total_net_pass_yds = int(data1[4][0]) + int(data1[4][1])\n",
    "print(total_net_pass_yds)\n",
    "total_tot_yds = int(data1[5][0]) + int(data1[5][1])\n",
    "print(total_tot_yds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', '0'], ['0', '0']]\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#seperate fumbles and make appropriate transformations\n",
    "fum_stats = [item.split('-') for item in data1[6]]\n",
    "print(fum_stats)\n",
    "total_fum = int(fum_stats[0][0]) + int(fum_stats[1][0])\n",
    "print(total_fum)\n",
    "total_fum_l = int(fum_stats[0][1]) + int(fum_stats[1][1])\n",
    "print(total_fum_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#turnover stats (doesn't require split)\n",
    "total_to = int(data1[7][0]) + int(data1[7][1])\n",
    "print(total_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['10', '71'], ['10', '107']]\n",
      "20\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "#seperate penalities & make appropriate transformations\n",
    "pen_stats = [item.split('-') for item in data1[8]]\n",
    "print(pen_stats)\n",
    "total_pen = int(pen_stats[0][0]) + int(pen_stats[1][0])\n",
    "print(total_pen)\n",
    "total_pen_yds = int(pen_stats[0][1]) + int(pen_stats[1][1])\n",
    "print(total_pen_yds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2', '12'], ['3', '15']]\n",
      "5\n",
      "27\n",
      "18.52\n"
     ]
    }
   ],
   "source": [
    "#seperate 3rd downs & make appropriate transformations\n",
    "third_dn_stats = [item.split('-') for item in data1[9]]\n",
    "print(third_dn_stats)\n",
    "total_third_conv = int(third_dn_stats[0][0]) + int(third_dn_stats[1][0])\n",
    "print(total_third_conv)\n",
    "total_thirds = int(third_dn_stats[0][1]) + int(third_dn_stats[1][1])\n",
    "print(total_thirds)\n",
    "total_third_per = round(100*total_third_conv/total_thirds,2)\n",
    "print(total_third_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', '0'], ['0', '2']]\n",
      "0\n",
      "2\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#seperate 4th downs & make appropriate transformations\n",
    "fourth_dn_stats = [item.split('-') for item in data1[10]]\n",
    "print(fourth_dn_stats)\n",
    "total_fourth_conv = int(fourth_dn_stats[0][0]) + int(fourth_dn_stats[1][0])\n",
    "print(total_fourth_conv)\n",
    "total_fourths = int(fourth_dn_stats[0][1]) + int(fourth_dn_stats[1][1])\n",
    "print(total_fourths)\n",
    "total_fourth_per = round(100*total_fourth_conv/total_fourths,2)\n",
    "print(total_fourth_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Efficiency Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-7.00',\n",
       "  '-20.02',\n",
       "  '-14.69',\n",
       "  '-4.34',\n",
       "  '-3.36',\n",
       "  '9.57',\n",
       "  '-2.26',\n",
       "  '10.15',\n",
       "  '0.00',\n",
       "  '-2.01',\n",
       "  '-2.21',\n",
       "  '1.83',\n",
       "  '4.70',\n",
       "  '-6.31',\n",
       "  '-0.02'],\n",
       " ['7.00',\n",
       "  '-9.57',\n",
       "  '2.26',\n",
       "  '-10.15',\n",
       "  '0.00',\n",
       "  '20.02',\n",
       "  '14.69',\n",
       "  '4.34',\n",
       "  '3.36',\n",
       "  '2.01',\n",
       "  '-1.83',\n",
       "  '2.21',\n",
       "  '6.31',\n",
       "  '-4.70',\n",
       "  '0.02']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape team stats box for efficiency metrics\n",
    "data2 = []\n",
    "table2 = soup.find('table', id='expected_points')\n",
    "table_body2 = table2.find('tbody')\n",
    "\n",
    "rows2 = table_body2.find_all('tr')\n",
    "for row in rows2:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data2.append([ele.text for ele in cols if ele]) # Get rid of empty values\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Efficiency Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "-29.59\n",
      "-12.43\n",
      "-14.49\n",
      "-3.36\n",
      "2.01\n"
     ]
    }
   ],
   "source": [
    "#grab relevant OFFESNIVE values & make appropriate transformations (defensive numbers are flipped sign)\n",
    "scoring_margin = int(abs(float(data2[0][0])))\n",
    "print(scoring_margin)\n",
    "total_off_epa = round(float(data2[0][1]) + float(data2[1][1]),2)\n",
    "print(total_off_epa)\n",
    "total_pass_epa = round(float(data2[0][2]) + float(data2[1][2]),2)\n",
    "print(total_pass_epa)\n",
    "total_rush_epa = round(float(data2[0][3]) + float(data2[1][3]),2)\n",
    "print(total_rush_epa)\n",
    "total_to_epa = round(float(data2[0][4]) + float(data2[1][4]),2)\n",
    "print(total_to_epa)\n",
    "total_spec_team_epa = abs(float(data2[0][9]))\n",
    "print(total_spec_team_epa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work Graveyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cirlce back for:\n",
    "- Play-by-play table to grab EPA's\n",
    "- Another source for other advanced stats (CPOE)\n",
    "- QB specific stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play-By-Play table, doesn't seem to pull in BS since it has weird \"comment\" section above it...\n",
    "#will cirlce back if time permits\n",
    "#REDEFINE DUMMY VARIABLES!!\n",
    "data2 = []\n",
    "table2 = soup.find('table', class_=\"sortable stats_table now_sortable sticky_table eq1 eq2 re2 le1\")\n",
    "table2\n",
    "table_body = table.find('<tbody>')\n",
    "\n",
    "rows = table_body.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data1.append([ele for ele in cols if ele]) # Get rid of empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trying to grab totals from last row of Scoring Plays Table\n",
    "#start with first game to get a sense of basic structure\n",
    "#response = requests.get(url_list[0])\n",
    "#page = response.text\n",
    "#soup = BeautifulSoup(page, 'html.parser')\n",
    "#Basic form to scrape any table\n",
    "#data = []\n",
    "#table = soup.find('table', attrs={'id':'scoring'})\n",
    "#table_body = table.find('tbody')\n",
    "\n",
    "#rows = table_body.find_all('tr')\n",
    "#for row in rows:\n",
    " #   cols = row.find_all('td')\n",
    " #   cols = [ele.text.strip() for ele in cols]\n",
    " #   data.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "#rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to use Comment module from bs4 package (didn't work; claimed Comment wasn't defined??)\n",
    "#to_remove = soup.find_all(text=Comment) \n",
    "#for element in to_remove: \n",
    "#    element.extract()\n",
    "\n",
    "#for comments in soup.findAll(text=lambda text:isinstance(text, Comment)):\n",
    "#    print(comments)\n",
    "#    comments.extract()\n",
    "\n",
    "#comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "#for c in comments:\n",
    "#    print(c)\n",
    "#    print(\"===========\")\n",
    "#    c.extract()\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
