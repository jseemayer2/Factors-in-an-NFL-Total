{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing team attributes, not teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usable Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell gathers URL ends that will be inputted into the subsequent cell in order to gather relevant data for EACH game that will be inputted into a Pandas DF object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "years = range(2011, 2020)\n",
    "weeks = range(1,22)\n",
    "#years = [2020]\n",
    "#weeks = [21]\n",
    "game_link_ends = []\n",
    "for year in years:\n",
    "    for week in weeks:\n",
    "        url0 = 'https://www.pro-football-reference.com/years/{}/week_{}.htm'.format(year, week)\n",
    "        response0 = requests.get(url0)\n",
    "        page0 = response0.text\n",
    "        soup0 = BeautifulSoup(page0)\n",
    "        for link in soup0.find_all(class_=\"right gamelink\"):\n",
    "            url1 = link.findNext()\n",
    "            game_link_ends0 = url1.get('href')  #.get() method and \n",
    "                                                #calling index by attr (url['href']) act the same!\n",
    "            game_link_ends.append(game_link_ends0.strip())\n",
    "#2020 season done separately since season incomplete\n",
    "weeks = range(1,20)\n",
    "for week in weeks:\n",
    "        url0 = 'https://www.pro-football-reference.com/years/2020/week_{}.htm'.format(week)\n",
    "        response0 = requests.get(url0)\n",
    "        page0 = response0.text\n",
    "        soup0 = BeautifulSoup(page0)\n",
    "        for link in soup0.find_all(class_=\"right gamelink\"):\n",
    "            url1 = link.findNext()\n",
    "            game_link_ends0 = url1.get('href')  #.get() method and \n",
    "                                                #calling index by attr (url['href']) act the same!\n",
    "            game_link_ends.append(game_link_ends0.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get to the actual game pages where we can extract data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://www.pro-football-reference.com'\n",
    "url_list = []\n",
    "for game_link in game_link_ends:\n",
    "    url = ''.join([url2, game_link])\n",
    "    url_list.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2669 2669\n"
     ]
    }
   ],
   "source": [
    "#Sanity checks to make sure we get all the boxscore links & subsequent pages (267 games/year)\n",
    "#print(*url_list, sep='\\n')\n",
    "print(len(url_list), len(game_link_ends))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are actually on the pages where we can grab info for a model, let's grab it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_stat_dict(url):\n",
    "    '''\n",
    "    From Pro-Football-Reference link stub, request gamepage, parse with BeautifulSoup, and\n",
    "    collect \n",
    "        - game total (target) \n",
    "        - traditional stats (features)\n",
    "        - efficiency stats (features)\n",
    "    Return total & stats as a dictionary.\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    page = page.replace(\"<!--\",\"\").replace(\"-->\",\"\")\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    headers = ['home','away','total','tot_1st',\n",
    "               'tot_rush_att','tot_rush_yds','tot_rush_tds',\n",
    "               'tot_comp','tot_att','tot_pass_yds','total_pass_tds','total_int',\n",
    "               'tot_sacks','tot_sack_yds','tot_net_pass_yds','total_tot_yds',\n",
    "               'tot_fum','tot_fum_l','total_to','tot_pen','tot_pen_yds',\n",
    "               'tot_third_conv','tot_thirds','tot_third_per',\n",
    "               'tot_fourth_conv','tot_fourths','tot_fourth_per',\n",
    "               'margin','tot_off_epa','tot_pass_epa','tot_rush_epa','tot_to_epa','tot_spec_epa']\n",
    "\n",
    "#scrape teams & totals from scorebox at top of page\n",
    "    teams = []\n",
    "    team_scores = []\n",
    "    table = soup.find('table', class_=\"linescore nohover stats_table no_freeze\")\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        #cols = [ele.text.strip() for ele in cols]\n",
    "        team_scores.append([ele for ele in cols[-1] if ele]) # Get rid of empty values\n",
    "        teams.append([ele.text for ele in cols[1] if ele])\n",
    "    home = teams[1][0]\n",
    "    away = teams[0][0]\n",
    "    total = sum([int(scores) for teams in team_scores for scores in teams]) #flatten list to be able to add\n",
    "\n",
    "#scrape team stats box for traditional metrics\n",
    "    data1 = []\n",
    "    table1 = soup.find('table', class_=\"add_controls stats_table\")\n",
    "    table_body1 = table1.find('tbody')\n",
    "\n",
    "    rows1 = table_body1.find_all('tr')\n",
    "    for row in rows1:\n",
    "        cols = row.find_all('td')\n",
    "        #cols = [ele.text.strip() for ele in cols]\n",
    "        data1.append([ele.text for ele in cols if ele]) # Get rid of empty values\n",
    "#process traditional stats table\n",
    "#total first downs\n",
    "    total_first_downs = sum(int(item) for item in data1[0])\n",
    "#separate rush stats & make appropriate transformations\n",
    "    rush_stats = [item.split('-') for item in data1[1]]\n",
    "    total_rush_att = int(rush_stats[0][0]) + int(rush_stats[1][0])\n",
    "    total_rush_yds = int(rush_stats[0][1]) + int(rush_stats[1][1])\n",
    "    total_rush_tds = int(rush_stats[0][2]) + int(rush_stats[1][2])\n",
    "#separate pass stats & make appropriate transformations\n",
    "    pass_stats = [item.split('-') for item in data1[2]]\n",
    "    total_comp = int(pass_stats[0][0]) + int(pass_stats[1][0])\n",
    "    total_att = int(pass_stats[0][1]) + int(pass_stats[1][1])\n",
    "    total_pass_yds = int(pass_stats[0][2]) + int(pass_stats[1][2])\n",
    "    total_pass_tds = int(pass_stats[0][3]) + int(pass_stats[1][3])\n",
    "    total_int = int(pass_stats[0][4]) + int(pass_stats[1][4])\n",
    "#separate sack stats & make appropriate transformations\n",
    "    sack_stats = [item.split('-') for item in data1[3]]\n",
    "    total_sacks = int(sack_stats[0][0]) + int(sack_stats[1][0])\n",
    "    total_sack_yds = int(sack_stats[0][1]) + int(sack_stats[1][1])\n",
    "#net pass yards & total yards (stats don't require splits)\n",
    "    total_net_pass_yds = int(data1[4][0]) + int(data1[4][1])\n",
    "    total_tot_yds = int(data1[5][0]) + int(data1[5][1])\n",
    "#separate fumbles & make appropriate transformations\n",
    "    fum_stats = [item.split('-') for item in data1[6]]\n",
    "    total_fum = int(fum_stats[0][0]) + int(fum_stats[1][0])\n",
    "    total_fum_l = int(fum_stats[0][1]) + int(fum_stats[1][1])\n",
    "#turnover stats (doesn't require split)\n",
    "    total_to = int(data1[7][0]) + int(data1[7][1])\n",
    "#separate penalty stats & make appropriate transformations\n",
    "    pen_stats = [item.split('-') for item in data1[8]]\n",
    "    total_pen = int(pen_stats[0][0]) + int(pen_stats[1][0])\n",
    "    total_pen_yds = int(pen_stats[0][1]) + int(pen_stats[1][1])\n",
    "#separate 3rd down stats & make appropriate transformations\n",
    "    third_dn_stats = [item.split('-') for item in data1[9]]\n",
    "    total_third_conv = int(third_dn_stats[0][0]) + int(third_dn_stats[1][0])\n",
    "    total_thirds = int(third_dn_stats[0][1]) + int(third_dn_stats[1][1])\n",
    "    total_third_per = round(100*total_third_conv/total_thirds,2)\n",
    "#separate 4th down stats & make appropriate transformations\n",
    "    fourth_dn_stats = [item.split('-') for item in data1[10]]\n",
    "    total_fourth_conv = int(fourth_dn_stats[0][0]) + int(fourth_dn_stats[1][0])\n",
    "    total_fourths = int(fourth_dn_stats[0][1]) + int(fourth_dn_stats[1][1])\n",
    "    if total_fourths != 0:\n",
    "        total_fourth_per = round(100*total_fourth_conv/total_fourths,2)\n",
    "    else:\n",
    "        total_fourth_per = None\n",
    "\n",
    "#scrape team stats box for efficiency metrics\n",
    "    data2 = []\n",
    "    table2 = soup.find('table', id='expected_points')\n",
    "    table_body2 = table2.find('tbody')\n",
    "\n",
    "    rows2 = table_body2.find_all('tr')\n",
    "    for row in rows2:\n",
    "        cols = row.find_all('td')\n",
    "        data2.append([ele.text for ele in cols if ele]) # Get rid of empty values\n",
    "#grab relevant OFFESNIVE values & make appropriate transformations (defensive numbers are flipped sign)\n",
    "    scoring_margin = int(abs(float(data2[0][0])))\n",
    "    total_off_epa = round(float(data2[0][1]) + float(data2[1][1]),2)\n",
    "    total_pass_epa = round(float(data2[0][2]) + float(data2[1][2]),2)\n",
    "    total_rush_epa = round(float(data2[0][3]) + float(data2[1][3]),2)\n",
    "    total_to_epa = round(float(data2[0][4]) + float(data2[1][4]),2)\n",
    "    total_spec_epa = abs(float(data2[0][9]))\n",
    "\n",
    "#make a dictionary of all the stats pulled from the gamepage\n",
    "    game_dict = dict(zip(headers,[home,away,total,total_first_downs,\n",
    "                                  total_rush_att,total_rush_yds,total_rush_tds,\n",
    "                                  total_comp,total_att,total_pass_yds,total_pass_tds,total_int,\n",
    "                                  total_sacks,total_sack_yds,total_net_pass_yds,total_tot_yds,\n",
    "                                  total_fum,total_fum_l,total_to,total_pen,total_pen_yds,\n",
    "                                  total_third_conv,total_thirds,total_third_per,\n",
    "                                  total_fourth_conv,total_fourths,total_fourth_per,\n",
    "                                  scoring_margin,total_off_epa,total_pass_epa,total_rush_epa,\n",
    "                                  total_to_epa,total_spec_epa]))\n",
    "    return game_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of the game stat dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_list = []\n",
    "\n",
    "for url in url_list:\n",
    "    game_list.append(game_stat_dict(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the list of dictionaries into a Pandas DataFrame object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home</th>\n",
       "      <th>away</th>\n",
       "      <th>total</th>\n",
       "      <th>tot_1st</th>\n",
       "      <th>tot_rush_att</th>\n",
       "      <th>tot_rush_yds</th>\n",
       "      <th>tot_rush_tds</th>\n",
       "      <th>tot_comp</th>\n",
       "      <th>tot_att</th>\n",
       "      <th>tot_pass_yds</th>\n",
       "      <th>...</th>\n",
       "      <th>tot_third_per</th>\n",
       "      <th>tot_fourth_conv</th>\n",
       "      <th>tot_fourths</th>\n",
       "      <th>tot_fourth_per</th>\n",
       "      <th>margin</th>\n",
       "      <th>tot_off_epa</th>\n",
       "      <th>tot_pass_epa</th>\n",
       "      <th>tot_rush_epa</th>\n",
       "      <th>tot_to_epa</th>\n",
       "      <th>tot_spec_epa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Green Bay Packers</td>\n",
       "      <td>New Orleans Saints</td>\n",
       "      <td>76</td>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "      <td>184</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>84</td>\n",
       "      <td>731</td>\n",
       "      <td>...</td>\n",
       "      <td>65.38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>43.00</td>\n",
       "      <td>38.99</td>\n",
       "      <td>4.54</td>\n",
       "      <td>-3.32</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>St. Louis Rams</td>\n",
       "      <td>Philadelphia Eagles</td>\n",
       "      <td>44</td>\n",
       "      <td>48</td>\n",
       "      <td>58</td>\n",
       "      <td>390</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>67</td>\n",
       "      <td>396</td>\n",
       "      <td>...</td>\n",
       "      <td>40.00</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18</td>\n",
       "      <td>8.39</td>\n",
       "      <td>3.99</td>\n",
       "      <td>7.93</td>\n",
       "      <td>-15.10</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baltimore Ravens</td>\n",
       "      <td>Pittsburgh Steelers</td>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "      <td>47</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>70</td>\n",
       "      <td>504</td>\n",
       "      <td>...</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>-12.06</td>\n",
       "      <td>-10.10</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>-29.55</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jacksonville Jaguars</td>\n",
       "      <td>Tennessee Titans</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>206</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>58</td>\n",
       "      <td>438</td>\n",
       "      <td>...</td>\n",
       "      <td>44.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.30</td>\n",
       "      <td>8.45</td>\n",
       "      <td>-5.50</td>\n",
       "      <td>-5.12</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Houston Texans</td>\n",
       "      <td>Indianapolis Colts</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>231</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>55</td>\n",
       "      <td>417</td>\n",
       "      <td>...</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27</td>\n",
       "      <td>-11.81</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>-8.81</td>\n",
       "      <td>-24.37</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   home                 away  total  tot_1st  tot_rush_att  \\\n",
       "0     Green Bay Packers   New Orleans Saints     76       54            48   \n",
       "1        St. Louis Rams  Philadelphia Eagles     44       48            58   \n",
       "2      Baltimore Ravens  Pittsburgh Steelers     42       37            47   \n",
       "3  Jacksonville Jaguars     Tennessee Titans     30       33            60   \n",
       "4        Houston Texans   Indianapolis Colts     41       41            57   \n",
       "\n",
       "   tot_rush_yds  tot_rush_tds  tot_comp  tot_att  tot_pass_yds  ...  \\\n",
       "0           184             2        59       84           731  ...   \n",
       "1           390             2        32       67           396  ...   \n",
       "2           236             1        39       70           504  ...   \n",
       "3           206             1        38       58           438  ...   \n",
       "4           231             2        33       55           417  ...   \n",
       "\n",
       "   tot_third_per  tot_fourth_conv  tot_fourths  tot_fourth_per  margin  \\\n",
       "0          65.38                0            1             0.0       8   \n",
       "1          40.00                2            4            50.0      18   \n",
       "2          40.00                0            2             0.0      27   \n",
       "3          44.83                0            0             NaN       2   \n",
       "4          30.00                0            1             0.0      27   \n",
       "\n",
       "   tot_off_epa  tot_pass_epa  tot_rush_epa  tot_to_epa  tot_spec_epa  \n",
       "0        43.00         38.99          4.54       -3.32          1.53  \n",
       "1         8.39          3.99          7.93      -15.10          0.21  \n",
       "2       -12.06        -10.10         -0.69      -29.55          3.57  \n",
       "3         3.30          8.45         -5.50       -5.12          2.37  \n",
       "4       -11.81         -1.75         -8.81      -24.37          8.14  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_df = pd.DataFrame(game_list) #convert list of dicts to DF\n",
    "game_df.head() #sanity check that we got all games and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2669, 33)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check that we got all the games and all the info we wanted\n",
    "game_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save pd object as a pickle file to load in our Regression notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('game_df.pickle', 'wb') as to_write:\n",
    "    pickle.dump(game_df, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check that all games went through the function\n",
    "print(len(game_list), len(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check link\n",
    "#url_list[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check stats\n",
    "#game_list[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check on dictionary key-value pairs\n",
    "headers = ['home','away','total','tot_1st',\n",
    "           'tot_rush_att','tot_rush_yds','tot_rush_tds',\n",
    "               'tot_comp','tot_att','tot_pass_yds','total_pass_tds','total_int',\n",
    "               'tot_sacks','tot_sack_yds','tot_net_pass_yds','total_tot_yds',\n",
    "               'tot_fum','tot_fum_l','total_to','tot_pen','tot_pen_yds',\n",
    "               'tot_third_conv','tot_thirds','tot_third_per',\n",
    "               'tot_fourth_conv','tot_fourths','tot_fourth_per',\n",
    "               'margin','tot_off_epa','tot_pass_epa','tot_rush_epa','tot_to_epa','tot_spec_epa']\n",
    "stats = [home, away,total,total_first_downs,\n",
    "         total_rush_att,total_rush_yds,total_rush_tds,\n",
    "         total_comp,total_att,total_pass_yds,total_pass_tds,total_int,\n",
    "         total_sacks,total_sack_yds,total_net_pass_yds,total_tot_yds,\n",
    "         total_fum,total_fum_l,total_to,total_pen,total_pen_yds,\n",
    "         total_third_conv,total_thirds,total_third_per,\n",
    "         total_fourth_conv,total_fourths,total_fourth_per,\n",
    "         scoring_margin,total_off_epa,total_pass_epa,total_rush_epa,\n",
    "         total_to_epa,total_spec_epa]\n",
    "\n",
    "len(headers) - len(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workable Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with first game to get a sense of basic structure; REMOVE COMMENTS\n",
    "response = requests.get(url_list[0])\n",
    "page = response.text\n",
    "page = page.replace(\"<!--\",\"\").replace(\"-->\",\"\")\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape teams & totals from scorebox at top of page\n",
    "teams = []\n",
    "data = []\n",
    "table = soup.find('table', class_=\"linescore nohover stats_table no_freeze\")\n",
    "table_body = table.find('tbody')\n",
    "\n",
    "rows = table_body.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data.append([ele for ele in cols[-1] if ele]) # Get rid of empty values\n",
    "    teams.append([ele.text for ele in cols[1] if ele])\n",
    "    \n",
    "total = sum([int(item) for sublist in data for item in sublist])\n",
    "total\n",
    "away = teams[0][0]\n",
    "home = teams[1][0]\n",
    "print(home, away, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.append([ele.text for ele in cols[1] if ele])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Traditional Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape team stats box for traditional metrics\n",
    "data1 = []\n",
    "table1 = soup.find('table', class_=\"add_controls stats_table\")\n",
    "table_body1 = table1.find('tbody')\n",
    "\n",
    "rows1 = table_body1.find_all('tr')\n",
    "for row in rows1:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data1.append([ele.text for ele in cols if ele]) # Get rid of empty values\n",
    "\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Traditional Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total first downs\n",
    "total_first_downs = sum(int(item) for item in data1[0])\n",
    "print(total_first_downs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate rush stats and make appropriate transformations\n",
    "rush_stats = [item.split('-') for item in data1[1]]\n",
    "total_rush_att = int(rush_stats[0][0]) + int(rush_stats[1][0])\n",
    "print(total_rush_att)\n",
    "total_rush_yds = int(rush_stats[0][1]) + int(rush_stats[1][1])\n",
    "print(total_rush_yds)\n",
    "total_rush_tds = int(rush_stats[0][2]) + int(rush_stats[1][2])\n",
    "print(total_rush_tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate pass stats and make appropriate transformations\n",
    "pass_stats = [item.split('-') for item in data1[2]]\n",
    "#print(pass_stats)\n",
    "total_comp = int(pass_stats[0][0]) + int(pass_stats[1][0])\n",
    "print(total_comp)\n",
    "total_att = int(pass_stats[0][1]) + int(pass_stats[1][1])\n",
    "print(total_att)\n",
    "total_pass_yds = int(pass_stats[0][2]) + int(pass_stats[1][2])\n",
    "print(total_pass_yds)\n",
    "total_pass_tds = int(pass_stats[0][3]) + int(pass_stats[1][3])\n",
    "print(total_pass_tds)\n",
    "total_int = int(pass_stats[0][4]) + int(pass_stats[1][4])\n",
    "print(total_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate sacks and make appropriate transformations\n",
    "sack_stats = [item.split('-') for item in data1[3]]\n",
    "#print(sack_stats)\n",
    "total_sacks = int(sack_stats[0][0]) + int(sack_stats[1][0])\n",
    "print(total_sacks)\n",
    "total_sack_yds = int(sack_stats[0][1]) + int(sack_stats[1][1])\n",
    "print(total_sack_yds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net pass yards & total yards (stats that don't require splits)\n",
    "total_net_pass_yds = int(data1[4][0]) + int(data1[4][1])\n",
    "print(total_net_pass_yds)\n",
    "total_tot_yds = int(data1[5][0]) + int(data1[5][1])\n",
    "print(total_tot_yds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate fumbles and make appropriate transformations\n",
    "fum_stats = [item.split('-') for item in data1[6]]\n",
    "print(fum_stats)\n",
    "total_fum = int(fum_stats[0][0]) + int(fum_stats[1][0])\n",
    "print(total_fum)\n",
    "total_fum_l = int(fum_stats[0][1]) + int(fum_stats[1][1])\n",
    "print(total_fum_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turnover stats (doesn't require split)\n",
    "total_to = int(data1[7][0]) + int(data1[7][1])\n",
    "print(total_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate penalities & make appropriate transformations\n",
    "pen_stats = [item.split('-') for item in data1[8]]\n",
    "print(pen_stats)\n",
    "total_pen = int(pen_stats[0][0]) + int(pen_stats[1][0])\n",
    "print(total_pen)\n",
    "total_pen_yds = int(pen_stats[0][1]) + int(pen_stats[1][1])\n",
    "print(total_pen_yds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate 3rd downs & make appropriate transformations\n",
    "third_dn_stats = [item.split('-') for item in data1[9]]\n",
    "print(third_dn_stats)\n",
    "total_third_conv = int(third_dn_stats[0][0]) + int(third_dn_stats[1][0])\n",
    "print(total_third_conv)\n",
    "total_thirds = int(third_dn_stats[0][1]) + int(third_dn_stats[1][1])\n",
    "print(total_thirds)\n",
    "total_third_per = round(100*total_third_conv/total_thirds,2)\n",
    "print(total_third_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate 4th downs & make appropriate transformations\n",
    "fourth_dn_stats = [item.split('-') for item in data1[10]]\n",
    "print(fourth_dn_stats)\n",
    "total_fourth_conv = int(fourth_dn_stats[0][0]) + int(fourth_dn_stats[1][0])\n",
    "print(total_fourth_conv)\n",
    "total_fourths = int(fourth_dn_stats[0][1]) + int(fourth_dn_stats[1][1])\n",
    "print(total_fourths)\n",
    "total_fourth_per = round(100*total_fourth_conv/total_fourths,2)\n",
    "print(total_fourth_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fourth_conv = 0\n",
    "total_fourths = 0\n",
    "if total_fourths != 0:\n",
    "    total_fourth_per = round(100*total_fourth_conv/total_fourths,2)\n",
    "else:\n",
    "    total_fourth_per = None\n",
    "print(total_fourth_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Efficiency Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape team stats box for efficiency metrics\n",
    "data2 = []\n",
    "table2 = soup.find('table', id='expected_points')\n",
    "table_body2 = table2.find('tbody')\n",
    "\n",
    "rows2 = table_body2.find_all('tr')\n",
    "for row in rows2:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data2.append([ele.text for ele in cols if ele]) # Get rid of empty values\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Efficiency Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab relevant OFFESNIVE values & make appropriate transformations (defensive numbers are flipped sign)\n",
    "scoring_margin = int(abs(float(data2[0][0])))\n",
    "print(scoring_margin)\n",
    "total_off_epa = round(float(data2[0][1]) + float(data2[1][1]),2)\n",
    "print(total_off_epa)\n",
    "total_pass_epa = round(float(data2[0][2]) + float(data2[1][2]),2)\n",
    "print(total_pass_epa)\n",
    "total_rush_epa = round(float(data2[0][3]) + float(data2[1][3]),2)\n",
    "print(total_rush_epa)\n",
    "total_to_epa = round(float(data2[0][4]) + float(data2[1][4]),2)\n",
    "print(total_to_epa)\n",
    "total_spec_epa = abs(float(data2[0][9]))\n",
    "print(total_spec_epa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work Graveyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cirlce back for:\n",
    "- Play-by-play table to grab EPA's\n",
    "- Another source for other advanced stats (CPOE)\n",
    "- QB specific stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play-By-Play table, doesn't seem to pull in BS since it has weird \"comment\" section above it...\n",
    "#will cirlce back if time permits\n",
    "#REDEFINE DUMMY VARIABLES!!\n",
    "data2 = []\n",
    "table2 = soup.find('table', class_=\"sortable stats_table now_sortable sticky_table eq1 eq2 re2 le1\")\n",
    "table2\n",
    "table_body = table.find('<tbody>')\n",
    "\n",
    "rows = table_body.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data1.append([ele for ele in cols if ele]) # Get rid of empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trying to grab totals from last row of Scoring Plays Table\n",
    "#start with first game to get a sense of basic structure\n",
    "#response = requests.get(url_list[0])\n",
    "#page = response.text\n",
    "#soup = BeautifulSoup(page, 'html.parser')\n",
    "#Basic form to scrape any table\n",
    "#data = []\n",
    "#table = soup.find('table', attrs={'id':'scoring'})\n",
    "#table_body = table.find('tbody')\n",
    "\n",
    "#rows = table_body.find_all('tr')\n",
    "#for row in rows:\n",
    " #   cols = row.find_all('td')\n",
    " #   cols = [ele.text.strip() for ele in cols]\n",
    " #   data.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "#rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to use Comment module from bs4 package (didn't work; claimed Comment wasn't defined??)\n",
    "#to_remove = soup.find_all(text=Comment) \n",
    "#for element in to_remove: \n",
    "#    element.extract()\n",
    "\n",
    "#for comments in soup.findAll(text=lambda text:isinstance(text, Comment)):\n",
    "#    print(comments)\n",
    "#    comments.extract()\n",
    "\n",
    "#comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "#for c in comments:\n",
    "#    print(c)\n",
    "#    print(\"===========\")\n",
    "#    c.extract()\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
