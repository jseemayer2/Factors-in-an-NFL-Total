{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing team attributes, not teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usable Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell gathers URL ends that will be inputted into the subsequent cell in order to gather relevant data for EACH game that will be inputted into a Pandas DF object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "years = range(2011, 2020)\n",
    "weeks = range(1,22)\n",
    "#years = [2020]\n",
    "#weeks = [21]\n",
    "game_link_ends = []\n",
    "for year in years:\n",
    "    for week in weeks:\n",
    "        url0 = 'https://www.pro-football-reference.com/years/{}/week_{}.htm'.format(year, week)\n",
    "        response0 = requests.get(url0)\n",
    "        page0 = response0.text\n",
    "        soup0 = BeautifulSoup(page0)\n",
    "        for link in soup0.find_all(class_=\"right gamelink\"):\n",
    "            url1 = link.findNext()\n",
    "            game_link_ends0 = url1.get('href')  #.get() method and \n",
    "                                                #calling index by attr (url['href']) act the same!\n",
    "            game_link_ends.append(game_link_ends0.strip())\n",
    "#2020 season done separately since season incomplete\n",
    "weeks = range(1,20)\n",
    "for week in weeks:\n",
    "        url0 = 'https://www.pro-football-reference.com/years/2020/week_{}.htm'.format(week)\n",
    "        response0 = requests.get(url0)\n",
    "        page0 = response0.text\n",
    "        soup0 = BeautifulSoup(page0)\n",
    "        for link in soup0.find_all(class_=\"right gamelink\"):\n",
    "            url1 = link.findNext()\n",
    "            game_link_ends0 = url1.get('href')  #.get() method and \n",
    "                                                #calling index by attr (url['href']) act the same!\n",
    "            game_link_ends.append(game_link_ends0.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get to the actual game pages where we can extract data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://www.pro-football-reference.com'\n",
    "url_list = []\n",
    "for game_link in game_link_ends:\n",
    "    url = ''.join([url2, game_link])\n",
    "    url_list.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2669 2669\n"
     ]
    }
   ],
   "source": [
    "#Sanity checks to make sure we get all the boxscore links & subsequent pages (267 games/year)\n",
    "#print(*url_list, sep='\\n')\n",
    "print(len(url_list), len(game_link_ends))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are actually on the pages where we can grab info for a model, let's grab it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_stat_dict(url):\n",
    "    '''\n",
    "    From Pro-Football-Reference link stub, request gamepage, parse with BeautifulSoup, and\n",
    "    collect \n",
    "        - game total (target) \n",
    "        - traditional stats (features)\n",
    "        - efficiency stats (features)\n",
    "    Return total & stats as a dictionary.\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    page = page.replace(\"<!--\",\"\").replace(\"-->\",\"\")\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    headers = ['home','away','total','tot_1st',\n",
    "               'tot_rush_att','tot_rush_yds','tot_rush_tds',\n",
    "               'tot_comp','tot_att','tot_pass_yds','total_pass_tds','total_int',\n",
    "               'tot_sacks','tot_sack_yds','tot_net_pass_yds','total_tot_yds',\n",
    "               'tot_fum','tot_fum_l','total_to','tot_pen','tot_pen_yds',\n",
    "               'tot_third_conv','tot_thirds','tot_third_per',\n",
    "               'tot_fourth_conv','tot_fourths','tot_fourth_per',\n",
    "               'margin','tot_off_epa','tot_pass_epa','tot_rush_epa','tot_to_epa','tot_spec_epa']\n",
    "\n",
    "#scrape teams & totals from scorebox at top of page\n",
    "    teams = []\n",
    "    team_scores = []\n",
    "    table = soup.find('table', class_=\"linescore nohover stats_table no_freeze\")\n",
    "    table_body = table.find('tbody')\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        #cols = [ele.text.strip() for ele in cols]\n",
    "        team_scores.append([ele for ele in cols[-1] if ele]) # Get rid of empty values\n",
    "        teams.append([ele.text for ele in cols[1] if ele])\n",
    "    home = teams[1][0]\n",
    "    away = teams[0][0]\n",
    "    total = sum([int(scores) for teams in team_scores for scores in teams]) #flatten list to be able to add\n",
    "\n",
    "#scrape team stats box for traditional metrics\n",
    "    data1 = []\n",
    "    table1 = soup.find('table', class_=\"add_controls stats_table\")\n",
    "    table_body1 = table1.find('tbody')\n",
    "\n",
    "    rows1 = table_body1.find_all('tr')\n",
    "    for row in rows1:\n",
    "        cols = row.find_all('td')\n",
    "        #cols = [ele.text.strip() for ele in cols]\n",
    "        data1.append([ele.text for ele in cols if ele]) # Get rid of empty values\n",
    "#process traditional stats table\n",
    "#total first downs\n",
    "    total_first_downs = sum(int(item) for item in data1[0])\n",
    "#separate rush stats & make appropriate transformations\n",
    "    rush_stats = [item.split('-') for item in data1[1]]\n",
    "    total_rush_att = int(rush_stats[0][0]) + int(rush_stats[1][0])\n",
    "    total_rush_yds = int(rush_stats[0][1]) + int(rush_stats[1][1])\n",
    "    total_rush_tds = int(rush_stats[0][2]) + int(rush_stats[1][2])\n",
    "#separate pass stats & make appropriate transformations\n",
    "    pass_stats = [item.split('-') for item in data1[2]]\n",
    "    total_comp = int(pass_stats[0][0]) + int(pass_stats[1][0])\n",
    "    total_att = int(pass_stats[0][1]) + int(pass_stats[1][1])\n",
    "    total_pass_yds = int(pass_stats[0][2]) + int(pass_stats[1][2])\n",
    "    total_pass_tds = int(pass_stats[0][3]) + int(pass_stats[1][3])\n",
    "    total_int = int(pass_stats[0][4]) + int(pass_stats[1][4])\n",
    "#separate sack stats & make appropriate transformations\n",
    "    sack_stats = [item.split('-') for item in data1[3]]\n",
    "    total_sacks = int(sack_stats[0][0]) + int(sack_stats[1][0])\n",
    "    total_sack_yds = int(sack_stats[0][1]) + int(sack_stats[1][1])\n",
    "#net pass yards & total yards (stats don't require splits)\n",
    "    total_net_pass_yds = int(data1[4][0]) + int(data1[4][1])\n",
    "    total_tot_yds = int(data1[5][0]) + int(data1[5][1])\n",
    "#separate fumbles & make appropriate transformations\n",
    "    fum_stats = [item.split('-') for item in data1[6]]\n",
    "    total_fum = int(fum_stats[0][0]) + int(fum_stats[1][0])\n",
    "    total_fum_l = int(fum_stats[0][1]) + int(fum_stats[1][1])\n",
    "#turnover stats (doesn't require split)\n",
    "    total_to = int(data1[7][0]) + int(data1[7][1])\n",
    "#separate penalty stats & make appropriate transformations\n",
    "    pen_stats = [item.split('-') for item in data1[8]]\n",
    "    total_pen = int(pen_stats[0][0]) + int(pen_stats[1][0])\n",
    "    total_pen_yds = int(pen_stats[0][1]) + int(pen_stats[1][1])\n",
    "#separate 3rd down stats & make appropriate transformations\n",
    "    third_dn_stats = [item.split('-') for item in data1[9]]\n",
    "    total_third_conv = int(third_dn_stats[0][0]) + int(third_dn_stats[1][0])\n",
    "    total_thirds = int(third_dn_stats[0][1]) + int(third_dn_stats[1][1])\n",
    "    total_third_per = round(100*total_third_conv/total_thirds,2)\n",
    "#separate 4th down stats & make appropriate transformations\n",
    "    fourth_dn_stats = [item.split('-') for item in data1[10]]\n",
    "    total_fourth_conv = int(fourth_dn_stats[0][0]) + int(fourth_dn_stats[1][0])\n",
    "    total_fourths = int(fourth_dn_stats[0][1]) + int(fourth_dn_stats[1][1])\n",
    "    if total_fourths != 0:\n",
    "        total_fourth_per = round(100*total_fourth_conv/total_fourths,2)\n",
    "    else:\n",
    "        total_fourth_per = None\n",
    "\n",
    "#scrape team stats box for efficiency metrics\n",
    "    data2 = []\n",
    "    table2 = soup.find('table', id='expected_points')\n",
    "    table_body2 = table2.find('tbody')\n",
    "\n",
    "    rows2 = table_body2.find_all('tr')\n",
    "    for row in rows2:\n",
    "        cols = row.find_all('td')\n",
    "        data2.append([ele.text for ele in cols if ele]) # Get rid of empty values\n",
    "#grab relevant OFFESNIVE values & make appropriate transformations (defensive numbers are flipped sign)\n",
    "    scoring_margin = int(abs(float(data2[0][0])))\n",
    "    total_off_epa = round(float(data2[0][1]) + float(data2[1][1]),2)\n",
    "    total_pass_epa = round(float(data2[0][2]) + float(data2[1][2]),2)\n",
    "    total_rush_epa = round(float(data2[0][3]) + float(data2[1][3]),2)\n",
    "    total_to_epa = round(float(data2[0][4]) + float(data2[1][4]),2)\n",
    "    total_spec_epa = abs(float(data2[0][9]))\n",
    "\n",
    "#make a dictionary of all the stats pulled from the gamepage\n",
    "    game_dict = dict(zip(headers,[home,away,total,total_first_downs,\n",
    "                                  total_rush_att,total_rush_yds,total_rush_tds,\n",
    "                                  total_comp,total_att,total_pass_yds,total_pass_tds,total_int,\n",
    "                                  total_sacks,total_sack_yds,total_net_pass_yds,total_tot_yds,\n",
    "                                  total_fum,total_fum_l,total_to,total_pen,total_pen_yds,\n",
    "                                  total_third_conv,total_thirds,total_third_per,\n",
    "                                  total_fourth_conv,total_fourths,total_fourth_per,\n",
    "                                  scoring_margin,total_off_epa,total_pass_epa,total_rush_epa,\n",
    "                                  total_to_epa,total_spec_epa]))\n",
    "    return game_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of the game stat dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_list = []\n",
    "\n",
    "for url in url_list:\n",
    "    game_list.append(game_stat_dict(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the list of dictionaries into a Pandas DataFrame object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df = pd.DataFrame(game_list) #convert list of dicts to DF\n",
    "game_df.head() #sanity check that we got all games and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check that we got all the games and all the info we wanted\n",
    "game_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save pd object as a pickle file to load in our Regression notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('game_df.pickle', 'wb') as to_write:\n",
    "    pickle.dump(game_df, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check that all games went through the function\n",
    "print(len(game_list), len(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check link\n",
    "#url_list[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check stats\n",
    "#game_list[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check on dictionary key-value pairs\n",
    "headers = ['home','away','total','tot_1st',\n",
    "           'tot_rush_att','tot_rush_yds','tot_rush_tds',\n",
    "               'tot_comp','tot_att','tot_pass_yds','total_pass_tds','total_int',\n",
    "               'tot_sacks','tot_sack_yds','tot_net_pass_yds','total_tot_yds',\n",
    "               'tot_fum','tot_fum_l','total_to','tot_pen','tot_pen_yds',\n",
    "               'tot_third_conv','tot_thirds','tot_third_per',\n",
    "               'tot_fourth_conv','tot_fourths','tot_fourth_per',\n",
    "               'margin','tot_off_epa','tot_pass_epa','tot_rush_epa','tot_to_epa','tot_spec_epa']\n",
    "stats = [home, away,total,total_first_downs,\n",
    "         total_rush_att,total_rush_yds,total_rush_tds,\n",
    "         total_comp,total_att,total_pass_yds,total_pass_tds,total_int,\n",
    "         total_sacks,total_sack_yds,total_net_pass_yds,total_tot_yds,\n",
    "         total_fum,total_fum_l,total_to,total_pen,total_pen_yds,\n",
    "         total_third_conv,total_thirds,total_third_per,\n",
    "         total_fourth_conv,total_fourths,total_fourth_per,\n",
    "         scoring_margin,total_off_epa,total_pass_epa,total_rush_epa,\n",
    "         total_to_epa,total_spec_epa]\n",
    "\n",
    "len(headers) - len(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workable Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with first game to get a sense of basic structure; REMOVE COMMENTS\n",
    "response = requests.get(url_list[0])\n",
    "page = response.text\n",
    "page = page.replace(\"<!--\",\"\").replace(\"-->\",\"\")\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape teams & totals from scorebox at top of page\n",
    "teams = []\n",
    "data = []\n",
    "table = soup.find('table', class_=\"linescore nohover stats_table no_freeze\")\n",
    "table_body = table.find('tbody')\n",
    "\n",
    "rows = table_body.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data.append([ele for ele in cols[-1] if ele]) # Get rid of empty values\n",
    "    teams.append([ele.text for ele in cols[1] if ele])\n",
    "    \n",
    "total = sum([int(item) for sublist in data for item in sublist])\n",
    "total\n",
    "away = teams[0][0]\n",
    "home = teams[1][0]\n",
    "print(home, away, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.append([ele.text for ele in cols[1] if ele])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Traditional Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape team stats box for traditional metrics\n",
    "data1 = []\n",
    "table1 = soup.find('table', class_=\"add_controls stats_table\")\n",
    "table_body1 = table1.find('tbody')\n",
    "\n",
    "rows1 = table_body1.find_all('tr')\n",
    "for row in rows1:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data1.append([ele.text for ele in cols if ele]) # Get rid of empty values\n",
    "\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Traditional Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total first downs\n",
    "total_first_downs = sum(int(item) for item in data1[0])\n",
    "print(total_first_downs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate rush stats and make appropriate transformations\n",
    "rush_stats = [item.split('-') for item in data1[1]]\n",
    "total_rush_att = int(rush_stats[0][0]) + int(rush_stats[1][0])\n",
    "print(total_rush_att)\n",
    "total_rush_yds = int(rush_stats[0][1]) + int(rush_stats[1][1])\n",
    "print(total_rush_yds)\n",
    "total_rush_tds = int(rush_stats[0][2]) + int(rush_stats[1][2])\n",
    "print(total_rush_tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate pass stats and make appropriate transformations\n",
    "pass_stats = [item.split('-') for item in data1[2]]\n",
    "#print(pass_stats)\n",
    "total_comp = int(pass_stats[0][0]) + int(pass_stats[1][0])\n",
    "print(total_comp)\n",
    "total_att = int(pass_stats[0][1]) + int(pass_stats[1][1])\n",
    "print(total_att)\n",
    "total_pass_yds = int(pass_stats[0][2]) + int(pass_stats[1][2])\n",
    "print(total_pass_yds)\n",
    "total_pass_tds = int(pass_stats[0][3]) + int(pass_stats[1][3])\n",
    "print(total_pass_tds)\n",
    "total_int = int(pass_stats[0][4]) + int(pass_stats[1][4])\n",
    "print(total_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate sacks and make appropriate transformations\n",
    "sack_stats = [item.split('-') for item in data1[3]]\n",
    "#print(sack_stats)\n",
    "total_sacks = int(sack_stats[0][0]) + int(sack_stats[1][0])\n",
    "print(total_sacks)\n",
    "total_sack_yds = int(sack_stats[0][1]) + int(sack_stats[1][1])\n",
    "print(total_sack_yds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net pass yards & total yards (stats that don't require splits)\n",
    "total_net_pass_yds = int(data1[4][0]) + int(data1[4][1])\n",
    "print(total_net_pass_yds)\n",
    "total_tot_yds = int(data1[5][0]) + int(data1[5][1])\n",
    "print(total_tot_yds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate fumbles and make appropriate transformations\n",
    "fum_stats = [item.split('-') for item in data1[6]]\n",
    "print(fum_stats)\n",
    "total_fum = int(fum_stats[0][0]) + int(fum_stats[1][0])\n",
    "print(total_fum)\n",
    "total_fum_l = int(fum_stats[0][1]) + int(fum_stats[1][1])\n",
    "print(total_fum_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turnover stats (doesn't require split)\n",
    "total_to = int(data1[7][0]) + int(data1[7][1])\n",
    "print(total_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate penalities & make appropriate transformations\n",
    "pen_stats = [item.split('-') for item in data1[8]]\n",
    "print(pen_stats)\n",
    "total_pen = int(pen_stats[0][0]) + int(pen_stats[1][0])\n",
    "print(total_pen)\n",
    "total_pen_yds = int(pen_stats[0][1]) + int(pen_stats[1][1])\n",
    "print(total_pen_yds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate 3rd downs & make appropriate transformations\n",
    "third_dn_stats = [item.split('-') for item in data1[9]]\n",
    "print(third_dn_stats)\n",
    "total_third_conv = int(third_dn_stats[0][0]) + int(third_dn_stats[1][0])\n",
    "print(total_third_conv)\n",
    "total_thirds = int(third_dn_stats[0][1]) + int(third_dn_stats[1][1])\n",
    "print(total_thirds)\n",
    "total_third_per = round(100*total_third_conv/total_thirds,2)\n",
    "print(total_third_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate 4th downs & make appropriate transformations\n",
    "fourth_dn_stats = [item.split('-') for item in data1[10]]\n",
    "print(fourth_dn_stats)\n",
    "total_fourth_conv = int(fourth_dn_stats[0][0]) + int(fourth_dn_stats[1][0])\n",
    "print(total_fourth_conv)\n",
    "total_fourths = int(fourth_dn_stats[0][1]) + int(fourth_dn_stats[1][1])\n",
    "print(total_fourths)\n",
    "total_fourth_per = round(100*total_fourth_conv/total_fourths,2)\n",
    "print(total_fourth_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fourth_conv = 0\n",
    "total_fourths = 0\n",
    "if total_fourths != 0:\n",
    "    total_fourth_per = round(100*total_fourth_conv/total_fourths,2)\n",
    "else:\n",
    "    total_fourth_per = None\n",
    "print(total_fourth_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Efficiency Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape team stats box for efficiency metrics\n",
    "data2 = []\n",
    "table2 = soup.find('table', id='expected_points')\n",
    "table_body2 = table2.find('tbody')\n",
    "\n",
    "rows2 = table_body2.find_all('tr')\n",
    "for row in rows2:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data2.append([ele.text for ele in cols if ele]) # Get rid of empty values\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Efficiency Stats Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab relevant OFFESNIVE values & make appropriate transformations (defensive numbers are flipped sign)\n",
    "scoring_margin = int(abs(float(data2[0][0])))\n",
    "print(scoring_margin)\n",
    "total_off_epa = round(float(data2[0][1]) + float(data2[1][1]),2)\n",
    "print(total_off_epa)\n",
    "total_pass_epa = round(float(data2[0][2]) + float(data2[1][2]),2)\n",
    "print(total_pass_epa)\n",
    "total_rush_epa = round(float(data2[0][3]) + float(data2[1][3]),2)\n",
    "print(total_rush_epa)\n",
    "total_to_epa = round(float(data2[0][4]) + float(data2[1][4]),2)\n",
    "print(total_to_epa)\n",
    "total_spec_epa = abs(float(data2[0][9]))\n",
    "print(total_spec_epa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work Graveyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cirlce back for:\n",
    "- Play-by-play table to grab EPA's\n",
    "- Another source for other advanced stats (CPOE)\n",
    "- QB specific stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play-By-Play table, doesn't seem to pull in BS since it has weird \"comment\" section above it...\n",
    "#will cirlce back if time permits\n",
    "#REDEFINE DUMMY VARIABLES!!\n",
    "data2 = []\n",
    "table2 = soup.find('table', class_=\"sortable stats_table now_sortable sticky_table eq1 eq2 re2 le1\")\n",
    "table2\n",
    "table_body = table.find('<tbody>')\n",
    "\n",
    "rows = table_body.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data1.append([ele for ele in cols if ele]) # Get rid of empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trying to grab totals from last row of Scoring Plays Table\n",
    "#start with first game to get a sense of basic structure\n",
    "#response = requests.get(url_list[0])\n",
    "#page = response.text\n",
    "#soup = BeautifulSoup(page, 'html.parser')\n",
    "#Basic form to scrape any table\n",
    "#data = []\n",
    "#table = soup.find('table', attrs={'id':'scoring'})\n",
    "#table_body = table.find('tbody')\n",
    "\n",
    "#rows = table_body.find_all('tr')\n",
    "#for row in rows:\n",
    " #   cols = row.find_all('td')\n",
    " #   cols = [ele.text.strip() for ele in cols]\n",
    " #   data.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "#rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to use Comment module from bs4 package (didn't work; claimed Comment wasn't defined??)\n",
    "#to_remove = soup.find_all(text=Comment) \n",
    "#for element in to_remove: \n",
    "#    element.extract()\n",
    "\n",
    "#for comments in soup.findAll(text=lambda text:isinstance(text, Comment)):\n",
    "#    print(comments)\n",
    "#    comments.extract()\n",
    "\n",
    "#comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "#for c in comments:\n",
    "#    print(c)\n",
    "#    print(\"===========\")\n",
    "#    c.extract()\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
