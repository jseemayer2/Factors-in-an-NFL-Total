{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell gathers URL ends that will be inputted into the subsequent cell in order to gather relevant data for EACH game that will be inputted into a Pandas DF object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/boxscores/201909050chi.htm',\n",
       " '/boxscores/201909080cle.htm',\n",
       " '/boxscores/201909080car.htm',\n",
       " '/boxscores/201909080phi.htm',\n",
       " '/boxscores/201909080nyj.htm',\n",
       " '/boxscores/201909080min.htm',\n",
       " '/boxscores/201909080mia.htm',\n",
       " '/boxscores/201909080jax.htm',\n",
       " '/boxscores/201909080sea.htm',\n",
       " '/boxscores/201909080sdg.htm',\n",
       " '/boxscores/201909080tam.htm',\n",
       " '/boxscores/201909080dal.htm',\n",
       " '/boxscores/201909080crd.htm',\n",
       " '/boxscores/201909080nwe.htm',\n",
       " '/boxscores/201909090nor.htm',\n",
       " '/boxscores/201909090rai.htm',\n",
       " '/boxscores/201909120car.htm',\n",
       " '/boxscores/201909150cin.htm',\n",
       " '/boxscores/201909150was.htm',\n",
       " '/boxscores/201909150rav.htm',\n",
       " '/boxscores/201909150pit.htm',\n",
       " '/boxscores/201909150oti.htm',\n",
       " '/boxscores/201909150nyg.htm',\n",
       " '/boxscores/201909150mia.htm',\n",
       " '/boxscores/201909150htx.htm',\n",
       " '/boxscores/201909150gnb.htm',\n",
       " '/boxscores/201909150det.htm',\n",
       " '/boxscores/201909150rai.htm',\n",
       " '/boxscores/201909150ram.htm',\n",
       " '/boxscores/201909150den.htm',\n",
       " '/boxscores/201909150atl.htm',\n",
       " '/boxscores/201909160nyj.htm']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = [2019]\n",
    "#years = range(2011, 2021)\n",
    "#weeks = range(1,22)\n",
    "weeks = [1,2]\n",
    "game_link_ends = []\n",
    "for year in years:\n",
    "    for week in weeks:\n",
    "        url0 = 'https://www.pro-football-reference.com/years/{}/week_{}.htm'.format(year, week)\n",
    "        response0 = requests.get(url0)\n",
    "        page0 = response0.text\n",
    "        soup0 = BeautifulSoup(page0)\n",
    "        for link in soup0.find_all(class_=\"right gamelink\"):\n",
    "            url1 = link.findNext()\n",
    "            game_link_ends0 = url1['href']\n",
    "            game_link_ends.append(game_link_ends0.strip())\n",
    "list(game_link_ends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get to the actual game pages where we can extract data into our Pandas DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://www.pro-football-reference.com'\n",
    "url_list = []\n",
    "for game_link in game_link_ends:\n",
    "    url = ''.join([url2, game_link])\n",
    "    url_list.append(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checks to make sure we get all the boxscore links & subsequent pages\n",
    "#print(*url_list, sep='\\n')\n",
    "#print(len(url_list), len(game_link_ends))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are actually on the pages where we can grab info for a model. Let's start with total points scored for the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 56,\n",
       " 57,\n",
       " 59,\n",
       " 33,\n",
       " 40,\n",
       " 69,\n",
       " 66,\n",
       " 41,\n",
       " 54,\n",
       " 48,\n",
       " 52,\n",
       " 54,\n",
       " 36,\n",
       " 58,\n",
       " 40,\n",
       " 34,\n",
       " 58,\n",
       " 52,\n",
       " 40,\n",
       " 54,\n",
       " 36,\n",
       " 42,\n",
       " 43,\n",
       " 25,\n",
       " 37,\n",
       " 23,\n",
       " 38,\n",
       " 36,\n",
       " 30,\n",
       " 44,\n",
       " 26]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start with first game to get a sense of basic structure\n",
    "totals = []\n",
    "for url in url_list:\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "#scrape totals from scorebox at top of page\n",
    "    data = []\n",
    "    table = soup.find('table', class_=\"linescore nohover stats_table no_freeze\")\n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        #cols = [ele.text.strip() for ele in cols]\n",
    "        data.append([ele for ele in cols[-1] if ele]) # Get rid of empty values\n",
    "        total = sum([int(item) for sublist in data for item in sublist])\n",
    "    totals.append(total)\n",
    "totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with first game to get a sense of basic structure\n",
    "response = requests.get(url_list[0])\n",
    "page = response.text\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "#Basic form to scrape any table\n",
    "data = []\n",
    "table = soup.find('table', attrs={'id':'scoring'})\n",
    "table_body = table.find('tbody')\n",
    "\n",
    "rows = table_body.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play-By-Play table, doesn't seem to pull in BS since it is rendered when naviagated to...\n",
    "#data2 = []\n",
    "#table2 = soup.find('table', class_=\"sortable stats_table now_sortable sticky_table eq1 eq2 re2 le1\")\n",
    "#table2\n",
    "#table_body = table.find('<tbody>')\n",
    "\n",
    "#rows = table_body.find_all('tr')\n",
    "#for row in rows:\n",
    "#    cols = row.find_all('td')\n",
    "#    cols = [ele.text.strip() for ele in cols]\n",
    "#    data1.append([ele for ele in cols if ele]) # Get rid of empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
