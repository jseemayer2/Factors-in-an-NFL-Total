{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing team attributes, not teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usable Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell gathers URL ends that will be inputted into the subsequent cell in order to gather relevant data for EACH game that will be inputted into a Pandas DF object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/boxscores/201909050chi.htm',\n",
       " '/boxscores/201909080cle.htm',\n",
       " '/boxscores/201909080car.htm',\n",
       " '/boxscores/201909080phi.htm',\n",
       " '/boxscores/201909080nyj.htm',\n",
       " '/boxscores/201909080min.htm',\n",
       " '/boxscores/201909080mia.htm',\n",
       " '/boxscores/201909080jax.htm',\n",
       " '/boxscores/201909080sea.htm',\n",
       " '/boxscores/201909080sdg.htm',\n",
       " '/boxscores/201909080tam.htm',\n",
       " '/boxscores/201909080dal.htm',\n",
       " '/boxscores/201909080crd.htm',\n",
       " '/boxscores/201909080nwe.htm',\n",
       " '/boxscores/201909090nor.htm',\n",
       " '/boxscores/201909090rai.htm',\n",
       " '/boxscores/201909120car.htm',\n",
       " '/boxscores/201909150cin.htm',\n",
       " '/boxscores/201909150was.htm',\n",
       " '/boxscores/201909150rav.htm',\n",
       " '/boxscores/201909150pit.htm',\n",
       " '/boxscores/201909150oti.htm',\n",
       " '/boxscores/201909150nyg.htm',\n",
       " '/boxscores/201909150mia.htm',\n",
       " '/boxscores/201909150htx.htm',\n",
       " '/boxscores/201909150gnb.htm',\n",
       " '/boxscores/201909150det.htm',\n",
       " '/boxscores/201909150rai.htm',\n",
       " '/boxscores/201909150ram.htm',\n",
       " '/boxscores/201909150den.htm',\n",
       " '/boxscores/201909150atl.htm',\n",
       " '/boxscores/201909160nyj.htm']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = [2019]\n",
    "#years = range(2011, 2021)\n",
    "#weeks = range(1,22)\n",
    "weeks = [1,2]\n",
    "game_link_ends = []\n",
    "for year in years:\n",
    "    for week in weeks:\n",
    "        url0 = 'https://www.pro-football-reference.com/years/{}/week_{}.htm'.format(year, week)\n",
    "        response0 = requests.get(url0)\n",
    "        page0 = response0.text\n",
    "        soup0 = BeautifulSoup(page0)\n",
    "        for link in soup0.find_all(class_=\"right gamelink\"):\n",
    "            url1 = link.findNext()\n",
    "            game_link_ends0 = url1.get('href')  #.get() method and \n",
    "                                                #calling index by attr (url['href']) act the same!\n",
    "            game_link_ends.append(game_link_ends0.strip())\n",
    "list(game_link_ends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get to the actual game pages where we can extract data into our Pandas DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://www.pro-football-reference.com'\n",
    "url_list = []\n",
    "for game_link in game_link_ends:\n",
    "    url = ''.join([url2, game_link])\n",
    "    url_list.append(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n"
     ]
    }
   ],
   "source": [
    "#Sanity checks to make sure we get all the boxscore links & subsequent pages\n",
    "#print(*url_list, sep='\\n')\n",
    "print(len(url_list), len(game_link_ends))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are actually on the pages where we can grab info for a model. Let's start with total points scored for the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USEABLE CODE BELOW, TEST SCRAPING ON ONE GAME FOR NOW!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 56,\n",
       " 57,\n",
       " 59,\n",
       " 33,\n",
       " 40,\n",
       " 69,\n",
       " 66,\n",
       " 41,\n",
       " 54,\n",
       " 48,\n",
       " 52,\n",
       " 54,\n",
       " 36,\n",
       " 58,\n",
       " 40,\n",
       " 34,\n",
       " 58,\n",
       " 52,\n",
       " 40,\n",
       " 54,\n",
       " 36,\n",
       " 42,\n",
       " 43,\n",
       " 25,\n",
       " 37,\n",
       " 23,\n",
       " 38,\n",
       " 36,\n",
       " 30,\n",
       " 44,\n",
       " 26]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start with first game to get a sense of basic structure\n",
    "totals = []\n",
    "for url in url_list:\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    page = page.replace(\"<!--\",\"\").replace(\"-->\",\"\")\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "#scrape totals from scorebox at top of page\n",
    "    data = []\n",
    "    table = soup.find('table', class_=\"linescore nohover stats_table no_freeze\")\n",
    "    table_body = table.find('tbody')\n",
    "\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        #cols = [ele.text.strip() for ele in cols]\n",
    "        data.append([ele for ele in cols[-1] if ele]) # Get rid of empty values\n",
    "        total = sum([int(item) for sublist in data for item in sublist]) #flatten list to be able to add\n",
    "    totals.append(total)\n",
    "totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with first game to get a sense of basic structure; REMOVE COMMENTS\n",
    "response = requests.get(url_list[0])\n",
    "page = response.text\n",
    "page = page.replace(\"<!--\",\"\").replace(\"-->\",\"\")\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape totals from scorebox at top of page\n",
    "data = []\n",
    "table = soup.find('table', class_=\"linescore nohover stats_table no_freeze\")\n",
    "table_body = table.find('tbody')\n",
    "\n",
    "rows = table_body.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data.append([ele for ele in cols[-1] if ele]) # Get rid of empty values\n",
    "    total = sum([int(item) for sublist in data for item in sublist])\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<td class=\"center\" data-stat=\"vis_stat\">13</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">16</td>],\n",
       " [<td class=\"center\" data-stat=\"vis_stat\">22-47-0</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">15-46-0</td>],\n",
       " [<td class=\"center\" data-stat=\"vis_stat\">18-30-203-1-0</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">26-45-228-0-1</td>],\n",
       " [<td class=\"center\" data-stat=\"vis_stat\">5-37</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">5-20</td>],\n",
       " [<td class=\"center\" data-stat=\"vis_stat\">166</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">208</td>],\n",
       " [<td class=\"center\" data-stat=\"vis_stat\">213</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">254</td>],\n",
       " [<td class=\"center\" data-stat=\"vis_stat\">1-0</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">0-0</td>],\n",
       " [<td class=\"center iz\" data-stat=\"vis_stat\">0</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">1</td>],\n",
       " [<td class=\"center\" data-stat=\"vis_stat\">10-71</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">10-107</td>],\n",
       " [<td class=\"center\" data-stat=\"vis_stat\">2-12</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">3-15</td>],\n",
       " [<td class=\"center\" data-stat=\"vis_stat\">0-0</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">0-2</td>],\n",
       " [<td class=\"center\" data-stat=\"vis_stat\">31:03</td>,\n",
       "  <td class=\"center\" data-stat=\"home_stat\">28:57</td>]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape team stats box for traditional metrics\n",
    "data1 = []\n",
    "table1 = soup.find('table', class_=\"add_controls stats_table\")\n",
    "table_body1 = table1.find('tbody')\n",
    "\n",
    "rows1 = table_body1.find_all('tr')\n",
    "for row in rows1:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data1.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<td class=\"right\" data-stat=\"pbp_exp_points_tot\">-7.00</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_off_tot\">-20.02</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_off_pass\">-14.69</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_off_rush\">-4.34</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_off_to\">-3.36</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_def_tot\">9.57</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_def_pass\">-2.26</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_def_rush\">10.15</td>,\n",
       "  <td class=\"right iz\" data-stat=\"pbp_exp_points_def_to\">0.00</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_st\">-2.01</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_k\">-2.21</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_kr\">1.83</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_p\">4.70</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_pr\">-6.31</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_fgxp\">-0.02</td>],\n",
       " [<td class=\"right\" data-stat=\"pbp_exp_points_tot\">7.00</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_off_tot\">-9.57</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_off_pass\">2.26</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_off_rush\">-10.15</td>,\n",
       "  <td class=\"right iz\" data-stat=\"pbp_exp_points_off_to\">0.00</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_def_tot\">20.02</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_def_pass\">14.69</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_def_rush\">4.34</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_def_to\">3.36</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_st\">2.01</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_k\">-1.83</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_kr\">2.21</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_p\">6.31</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_pr\">-4.70</td>,\n",
       "  <td class=\"right\" data-stat=\"pbp_exp_points_fgxp\">0.02</td>]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrape team stats box for efficiency metrics\n",
    "data2 = []\n",
    "table2 = soup.find('table', id='expected_points')\n",
    "table_body2 = table2.find('tbody')\n",
    "\n",
    "rows2 = table_body2.find_all('tr')\n",
    "for row in rows2:\n",
    "    cols = row.find_all('td')\n",
    "    #cols = [ele.text.strip() for ele in cols]\n",
    "    data2.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Work Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trying to grab totals from last row of Scoring Play Table\n",
    "#start with first game to get a sense of basic structure\n",
    "#response = requests.get(url_list[0])\n",
    "#page = response.text\n",
    "#soup = BeautifulSoup(page, 'html.parser')\n",
    "#Basic form to scrape any table\n",
    "#data = []\n",
    "#table = soup.find('table', attrs={'id':'scoring'})\n",
    "#table_body = table.find('tbody')\n",
    "\n",
    "#rows = table_body.find_all('tr')\n",
    "#for row in rows:\n",
    " #   cols = row.find_all('td')\n",
    " #   cols = [ele.text.strip() for ele in cols]\n",
    " #   data.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "#rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to use Comment module from bs4 package (didn't work; claimed Comment wasn't defined??)\n",
    "#to_remove = soup.find_all(text=Comment) \n",
    "#for element in to_remove: \n",
    "#    element.extract()\n",
    "\n",
    "#for comments in soup.findAll(text=lambda text:isinstance(text, Comment)):\n",
    "#    print(comments)\n",
    "#    comments.extract()\n",
    "\n",
    "#comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "#for c in comments:\n",
    "#    print(c)\n",
    "#    print(\"===========\")\n",
    "#    c.extract()\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play-By-Play table, doesn't seem to pull in BS since it has weird \"comment\" section above it...\n",
    "#will cirlce back if time permits\n",
    "data2 = []\n",
    "table2 = soup.find('table', class_=\"sortable stats_table now_sortable sticky_table eq1 eq2 re2 le1\")\n",
    "table2\n",
    "table_body = table.find('<tbody>')\n",
    "\n",
    "rows = table_body.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data1.append([ele for ele in cols if ele]) # Get rid of empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to remove commented section\n",
    "#page.replace('<!--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
